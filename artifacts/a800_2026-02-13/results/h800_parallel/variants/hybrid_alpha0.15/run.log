[train:hybrid_alpha0.15] n_seq=24,414 total_steps=382/382 micro_batch=64 grad_accum=1
[train:hybrid_alpha0.15] step=1/382 loss=10.9517 lr=0.000e+00 tokens=131,072
[train:hybrid_alpha0.15] step=20/382 loss=6.1406 lr=5.985e-04 tokens=131,072
[train:hybrid_alpha0.15] step=40/382 loss=5.7051 lr=5.893e-04 tokens=131,072
[train:hybrid_alpha0.15] step=60/382 loss=5.3867 lr=5.720e-04 tokens=131,072
[train:hybrid_alpha0.15] step=80/382 loss=4.9210 lr=5.471e-04 tokens=131,072
[train:hybrid_alpha0.15] step=100/382 loss=4.5095 lr=5.152e-04 tokens=131,072
[train:hybrid_alpha0.15] step=120/382 loss=4.1511 lr=4.774e-04 tokens=131,072
[train:hybrid_alpha0.15] step=140/382 loss=3.9230 lr=4.345e-04 tokens=131,072
[train:hybrid_alpha0.15] step=160/382 loss=3.7595 lr=3.879e-04 tokens=131,072
[train:hybrid_alpha0.15] step=180/382 loss=3.5714 lr=3.388e-04 tokens=131,072
[train:hybrid_alpha0.15] step=200/382 loss=3.3789 lr=2.887e-04 tokens=131,072
[train:hybrid_alpha0.15] step=220/382 loss=3.2724 lr=2.389e-04 tokens=131,072
[train:hybrid_alpha0.15] step=240/382 loss=3.2149 lr=1.907e-04 tokens=131,072
[train:hybrid_alpha0.15] step=260/382 loss=3.1533 lr=1.457e-04 tokens=131,072
[train:hybrid_alpha0.15] step=280/382 loss=3.1302 lr=1.049e-04 tokens=131,072
[train:hybrid_alpha0.15] step=300/382 loss=3.0394 lr=6.965e-05 tokens=131,072
[train:hybrid_alpha0.15] step=320/382 loss=2.9748 lr=4.082e-05 tokens=131,072
[train:hybrid_alpha0.15] step=340/382 loss=2.9459 lr=1.926e-05 tokens=131,072
[train:hybrid_alpha0.15] step=360/382 loss=2.9754 lr=5.552e-06 tokens=131,072
[train:hybrid_alpha0.15] step=380/382 loss=2.9234 lr=9.474e-08 tokens=131,072
[eval] L= 2048 PPL=18.067 ± 2.268
[eval] L=16384 PPL=60.334 ± 4.499
{
  "name": "hybrid_alpha0.15",
  "ppl@16384": 60.33377413759333
}
