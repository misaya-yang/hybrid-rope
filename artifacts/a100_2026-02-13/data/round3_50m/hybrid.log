============================================================
  HYBRID EXPERIMENTS: Mixing geo_10k + anchpoly_p3.9
============================================================
  Loading TinyStories (train)...
  Got 24414 chunks (50.0M tokens)
  Loading TinyStories (validation)...
  Got 290 chunks (4.8M tokens)

>>> Baseline: geo_10k

============================================================
  EXPERIMENT: geo_10k_baseline
============================================================
  inv_freq: [1.33e-04, 1.00e+00]
  Model: 50.9M params
  Training: 762 steps
    step 100/762  loss=6.0842  lr=2.91e-04  ETA=3min
    step 200/762  loss=3.5153  lr=2.57e-04  ETA=3min
    step 300/762  loss=2.9225  lr=2.05e-04  ETA=2min
    step 400/762  loss=2.6758  lr=1.43e-04  ETA=2min
    step 500/762  loss=2.5312  lr=8.28e-05  ETA=1min
    step 600/762  loss=2.4404  lr=3.39e-05  ETA=1min
    step 700/762  loss=2.4131  lr=5.23e-06  ETA=0min
  Saved: /opt/dfrope/results/round3_50m/geo_10k_baseline.pt
  Evaluating...
      2048: PPL = 8.40
      4096: PPL = 8.48
      8192: PPL = 10.01
     12288: PPL = 13.07
     16384: PPL = 15.72

================================================================================
  HYBRID RESULTS TABLE (vs geo_10k)
================================================================================
Config                         |     2048 |    16384 |   vs geo_10k
--------------------------------------------------------------------------------
geo_10k_baseline               |     8.40 |    15.72 |        +0.0%
================================================================================

>>> Hybrid Experiments

============================================================
  EXPERIMENT: hybrid_alpha0.1
============================================================
  inv_freq: [1.24e-04, 1.00e+00]
  Model: 50.9M params
  Training: 762 steps
    step 100/762  loss=6.0980  lr=2.91e-04  ETA=3min
    step 200/762  loss=3.5070  lr=2.57e-04  ETA=3min
    step 300/762  loss=2.9142  lr=2.05e-04  ETA=2min
    step 400/762  loss=2.6676  lr=1.43e-04  ETA=2min
    step 500/762  loss=2.5251  lr=8.28e-05  ETA=1min
    step 600/762  loss=2.4348  lr=3.39e-05  ETA=1min
    step 700/762  loss=2.4072  lr=5.23e-06  ETA=0min
  Saved: /opt/dfrope/results/round3_50m/hybrid_alpha0.1.pt
  Evaluating...
      2048: PPL = 8.36
      4096: PPL = 8.34
      8192: PPL = 9.51
     12288: PPL = 12.32
     16384: PPL = 13.73

================================================================================
  HYBRID RESULTS TABLE (vs geo_10k)
================================================================================
Config                         |     2048 |    16384 |   vs geo_10k
--------------------------------------------------------------------------------
geo_10k_baseline               |     8.40 |    15.72 |        +0.0%
hybrid_alpha0.1                |     8.36 |    13.73 |       +12.7% ðŸ‘‘
================================================================================

============================================================
  EXPERIMENT: hybrid_alpha0.2
============================================================
  inv_freq: [1.15e-04, 1.00e+00]
  Model: 50.9M params
  Training: 762 steps
    step 100/762  loss=6.1812  lr=2.91e-04  ETA=3min
    step 200/762  loss=3.5728  lr=2.57e-04  ETA=3min
    step 300/762  loss=2.9413  lr=2.05e-04  ETA=2min
    step 400/762  loss=2.6847  lr=1.43e-04  ETA=2min
    step 500/762  loss=2.5395  lr=8.28e-05  ETA=1min
    step 600/762  loss=2.4486  lr=3.39e-05  ETA=1min
    step 700/762  loss=2.4209  lr=5.23e-06  ETA=0min
  Saved: /opt/dfrope/results/round3_50m/hybrid_alpha0.2.pt
  Evaluating...
      2048: PPL = 8.51
      4096: PPL = 8.48
      8192: PPL = 9.26
     12288: PPL = 11.24
     16384: PPL = 12.35

================================================================================
  HYBRID RESULTS TABLE (vs geo_10k)
================================================================================
Config                         |     2048 |    16384 |   vs geo_10k
--------------------------------------------------------------------------------
geo_10k_baseline               |     8.40 |    15.72 |        +0.0%
hybrid_alpha0.1                |     8.36 |    13.73 |       +12.7% ðŸ‘‘
hybrid_alpha0.2                |     8.51 |    12.35 |       +21.5% ðŸ‘‘
================================================================================

============================================================
  EXPERIMENT: hybrid_alpha0.3
============================================================
  inv_freq: [1.05e-04, 1.00e+00]
  Model: 50.9M params
  Training: 762 steps
    step 100/762  loss=6.1380  lr=2.91e-04  ETA=3min
    step 200/762  loss=3.5336  lr=2.57e-04  ETA=3min
    step 300/762  loss=2.9283  lr=2.05e-04  ETA=2min
    step 400/762  loss=2.6792  lr=1.43e-04  ETA=2min
    step 500/762  loss=2.5374  lr=8.28e-05  ETA=1min
    step 600/762  loss=2.4467  lr=3.39e-05  ETA=1min
    step 700/762  loss=2.4188  lr=5.23e-06  ETA=0min
  Saved: /opt/dfrope/results/round3_50m/hybrid_alpha0.3.pt
  Evaluating...
      2048: PPL = 8.50
      4096: PPL = 8.49
      8192: PPL = 9.47
     12288: PPL = 12.00
     16384: PPL = 13.25

================================================================================
  HYBRID RESULTS TABLE (vs geo_10k)
================================================================================
Config                         |     2048 |    16384 |   vs geo_10k
--------------------------------------------------------------------------------
geo_10k_baseline               |     8.40 |    15.72 |        +0.0%
hybrid_alpha0.1                |     8.36 |    13.73 |       +12.7% ðŸ‘‘
hybrid_alpha0.2                |     8.51 |    12.35 |       +21.5% ðŸ‘‘
hybrid_alpha0.3                |     8.50 |    13.25 |       +15.8% ðŸ‘‘
================================================================================

============================================================
  EXPERIMENT: hybrid_alpha0.4
============================================================
  inv_freq: [9.60e-05, 1.00e+00]
  Model: 50.9M params
  Training: 762 steps
    step 100/762  loss=6.1136  lr=2.91e-04  ETA=3min
    step 200/762  loss=3.5160  lr=2.57e-04  ETA=3min
    step 300/762  loss=2.9238  lr=2.05e-04  ETA=2min
    step 400/762  loss=2.6801  lr=1.43e-04  ETA=2min
    step 500/762  loss=2.5428  lr=8.28e-05  ETA=1min
    step 600/762  loss=2.4558  lr=3.39e-05  ETA=1min
    step 700/762  loss=2.4296  lr=5.23e-06  ETA=0min
  Saved: /opt/dfrope/results/round3_50m/hybrid_alpha0.4.pt
  Evaluating...
      2048: PPL = 8.57
      4096: PPL = 8.56
      8192: PPL = 10.02
     12288: PPL = 13.23
     16384: PPL = 14.79

================================================================================
  HYBRID RESULTS TABLE (vs geo_10k)
================================================================================
Config                         |     2048 |    16384 |   vs geo_10k
--------------------------------------------------------------------------------
geo_10k_baseline               |     8.40 |    15.72 |        +0.0%
hybrid_alpha0.1                |     8.36 |    13.73 |       +12.7% ðŸ‘‘
hybrid_alpha0.2                |     8.51 |    12.35 |       +21.5% ðŸ‘‘
hybrid_alpha0.3                |     8.50 |    13.25 |       +15.8% ðŸ‘‘
hybrid_alpha0.4                |     8.57 |    14.79 |        +5.9% ðŸ‘‘
================================================================================

============================================================
  EXPERIMENT: hybrid_alpha0.5
============================================================
  inv_freq: [8.67e-05, 1.00e+00]
  Model: 50.9M params
  Training: 762 steps
    step 100/762  loss=6.1325  lr=2.91e-04  ETA=3min
    step 200/762  loss=3.5322  lr=2.57e-04  ETA=3min
