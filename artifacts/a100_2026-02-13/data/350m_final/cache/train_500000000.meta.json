{
  "dataset": "roneneldan/TinyStories",
  "split": "train",
  "tokenizer": "EleutherAI/gpt-neox-20b",
  "target_tokens": 500000000,
  "seq_len": 2048,
  "written_tokens": 472770254,
  "usable_tokens": 472768512,
  "usable_chunks": 230844,
  "path": "/opt/dfrope/results/350m_final/cache/train_500000000.tokens.u16",
  "dtype": "uint16",
  "created_at": "2026-02-12_115747"
}